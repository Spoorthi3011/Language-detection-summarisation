# -*- coding: utf-8 -*-
"""LANGUAGE DETECTION-SLP-1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hbbuVAU-XaxiO0YGlxw_qONW7Pd2dJrU
"""

!unzip "/content/archive (9).zip"

import pandas as pd
import numpy as np
import re
import seaborn as sns
import matplotlib.pyplot as plt
import pickle


import warnings
warnings.simplefilter("ignore")

data = pd.read_csv("/content/Language Detection.csv")

data.head(10)

data["Language"].value_counts()

# separating the independent and dependant features
X = data["Text"]
y = data["Language"]

"""TEXT PREPROCESSING"""

# converting categorical variables to numerical

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)

data_list = []
for text in X:
    text = re.sub(r'[!@#$(),\n"%^*?\:;~`0-9]', ' ', text)
    text = re.sub(r'[[]]', ' ', text)
    text = text.lower()
    data_list.append(text)

# creating bag of words using countvectorizer

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
X = cv.fit_transform(data_list).toarray()

X.shape

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(x_train, y_train)

# prediction
y_pred = model.predict(x_test)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

ac = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
cr = classification_report(y_test, y_pred)

print("Accuracy is :",ac)

# classification report
print(cr)

# visualising the confusion matrix
plt.figure(figsize=(15,10))
sns.heatmap(cm, annot = True)
plt.show()

data.info()

data.isnull().sum()

data[data.duplicated()]

len(data[data.duplicated()])

data.drop(data[data.duplicated()].index, axis=0, inplace=True)

data.shape

data["Language"].nunique()

plt.figure(figsize=(20,8))

total= float(len(data['Language']))
ax= sns.countplot(x= 'Language', data= data, order= data['Language'].value_counts().index, palette= 'magma')

for p in ax.patches:
    percentage= '{:.2f}%'.format(100 * p.get_height()/total)
    x= p.get_x() + p.get_width()
    y= p.get_height()
    ax.annotate(percentage, (x, y), fontsize=16, ha='center')

plt.title('Counts and Percentages of Languages', fontsize=24)
plt.xlabel("Language",fontsize=20)
plt.ylabel("Count", fontsize=20)
plt.xticks(size= 18, rotation=90)
plt.show()

language= data['Language'].value_counts().reset_index()
language

import matplotlib.pyplot as plt

plt.figure(figsize=(10,10))

labels = language['Language']

# Create the pie chart
plt.pie(language["count"], labels=labels, autopct='%.1f%%', textprops={'fontsize': 15})

plt.show()

df1= data.copy()
df1['cleaned_Text']= ""
df1

import re
def clean_function(Text):
    Text = re.sub(r'[\([{})\]!@#$,"%^*?:;~`0-9]', ' ', Text)

    Text = Text.lower()
    Text = re.sub('http\S+\s*', ' ', Text)
    Text = re.sub('RT|cc', ' ', Text)
    Text = re.sub('#\S+', '', Text)
    Text = re.sub('@\S+', '  ', Text)
    Text = re.sub('\s+', ' ', Text)

    return Text

df1['cleaned_Text'] = df1['Text'].apply(lambda x: clean_function(x))
df1

X= df1["cleaned_Text"]
y= df1["Language"]

from sklearn.preprocessing import LabelEncoder

encoder= LabelEncoder()

y= encoder.fit_transform(y)

from sklearn.feature_extraction.text import CountVectorizer
CV= CountVectorizer()
X= CV.fit_transform(X).toarray()
X.shape

#splitting
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(X, y, random_state=42)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB

models = {
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Multinomial Naive Bayes': MultinomialNB(),
}

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for name, model in models.items():
#     print(f'{name} training started...')
#     model.fit(X_train, y_train)
#     print(f'{name} trained')

#Model Evaluation
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix as CM
from sklearn.metrics import classification_report

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for name in models:
#     print(f'{name} accuracy score :  {round(accuracy_score(y_test, models.get(name).predict(X_test)), 3)}')
#     acc_score= round(accuracy_score(y_test, models.get(name).predict(X_test)), 3)
#     print(f'{name} accuracy score :  {acc_score}')

for name in models:
    print(f'{name} classification report')
    print("-------------------------------")
    print(classification_report(y_test, models.get(name).predict(X_test)))
    print("******************************")
    print(" ")

for name in models:
    print(f'{name} ConfusionMatrix')
    predictions= models.get(name).predict(X_test)
    score = round(accuracy_score(y_test, models.get(name).predict(X_test)), 3)
    confusionMatrix = CM(y_test, models.get(name).predict(X_test))
    sns.heatmap(confusionMatrix, annot=True, fmt=".0f")
    plt.xlabel('Actual Values')
    plt.ylabel('Prediction Values')
    plt.title('Accuracy Score: {0}'.format(score), size = 15)
    plt.show()
    print(" ")
    print(" ")

#prediction
def prediction(text):
    x= CV.transform([text]).toarray()
    lang= model.predict(x)
    lang= encoder.inverse_transform(lang)
    print("This word/sentence contains {} word(s).".format(lang[0]))

prediction("passe une bonne journée")

prediction("Nimal")

prediction("நிமல்")

prediction("Pri unum aliquam definitionem ei, dicant reprehendunt cu nec. Intellegat accommodare ea usu. Ius illum delenit quaerendum ei, pri assum nobis id, iriure deseruisse deterruisset ad nam")
